---
title: "Homework 4"
author: Yue Guo
date: February 15, 2022
output: github_document
---


# Question 1

## a
1. When x <= 0.95 and x >= 0.05
   p =10%
2. When x < 0.05,the sample is between [0,x+0.05]
   p = (100x + 5)%
3. When x > 0.95,the sample is between [x - 0.05,1]
   p = (105 - 100x)%

In conclusion:
$\int_{0.05}^{0.95}10dx + \int_{0}^{0.05}(100x + 5)dx + \int_{0.95}^{1}(105-100x)dx = 9.75$
   
   
p = 9.75%

## b
9.75%^2 = 0.950625%

## c
9.75%^100

## d
From the answer of a to c, we can find that when the number of the features is increasing, the number of pbservations we can use to predict is decreasing, which will makes the prediction result worse. When the number of features is large enough, there will be no obersvations close to the given test and aslo no predictions. 

## e
p =1
```{r}
x = 0.1
x
```

p = 2
```{r}
x = 0.1^(1/2)
x
```


x = sqrt(0.1)

p = 100

```{r}
x = 0.1^(1/100)
x
```

# Question 2
## a
```{r}
library(ISLR2)
library(corrplot)
library(tidyverse)
library(MASS)
library(class)
library(e1071)

```

```{r}
summary(Weekly)
attach(Weekly)
cordata <- cor(Weekly[,-9])
corrplot(cordata)
```

## b
```{r}
l_model <- glm(Direction ~ Lag1+Lag2+Lag3+Lag4+Lag5+Volume,data = Weekly,family=binomial)

summary(l_model)

```
from the result we cna see that Lag2 id statistically significant.
## c
```{r}
result <- function(fun,data){
  prob <- predict(fun,data)
  result_prediction = rep("Down", length(prob))
  result_prediction[prob > 0.5] <- "Up"
  tab <- table(data$Direction,result_prediction)
  return(tab)
}
fraction <- function(table){
  acc <- (table[1] + table[4])/(table[2] + table[3]+table[1] + table[4])
  return(acc)
}
table <- result(l_model,Weekly)
table
fraction(table)
```

There are two kind of mistakes:
1. Predict Down which in fact is up: 
```{r}
 563/(563+42)
```
2. Predict Up which in fact is Down: 
```{r}
19/(465+19)
```

## d
```{r}
train_data <- Weekly%>%
  filter(Year >= 1990 & Year < 2008)

test_data <-Weekly%>%
  filter(Year >= 2009 & Year < 2010)

l_model_new <- glm(Direction ~ Lag2,data = train_data,family=binomial)


result_glm <- result(l_model_new, test_data)
result_glm
fraction(result_glm)
```
Overall fraction is 0.4230769

## e
```{r}
model_LDA <- lda(Direction~Lag2, data=train_data,family=binomial)
LDA <- predict(model_LDA,test_data)$class
result_LDA <- table(test_data$Direction,LDA)
result_LDA
fraction(result_LDA)
```
Overall fraction is 0.5384615
## f
```{r}
model_QDA <- qda(Direction~Lag2, data=train_data,family=binomial)
QDA <- predict(model_QDA,test_data)$class
result_QDA <- table(test_data$Direction,QDA)
result_QDA
fraction(result_QDA)
```
Overall fraction is 0.5576923
## g
```{r}
Week.train <- as.matrix(train_data["Lag2"])
Week.test <- as.matrix(test_data["Lag2"])
train.Direction <- as.matrix(train_data["Direction"])
set.seed(1)
Weekknn.pred <- knn(Week.train,Week.test,train.Direction,k=1)
Weekknn_table <- table(Weekknn.pred,as.matrix(test_data["Direction"]))
Weekknn_table
fraction(Weekknn_table)
```
Overall fraction is 0.5384615
## h
```{r}
model_nb <- naiveBayes(Week.train,train.Direction)
Weekknn_pred_nb <- predict(model_nb,Week.test)
Weekknn_table_nb <- table(Weekknn_pred_nb,as.matrix(test_data["Direction"]))

Weekknn_table_nb
fraction(Weekknn_table_nb)
```
Overall fraction is 0.5576923
## i
From the result we can see that QDA and naive bayes has the best result.

## j
I will use Lag2 +Volume and K =2,3,4 experiment.

### logistic regression
```{r}
lg_model <- glm(Direction ~ Lag2 + Volume +Lag1, data = train_data,family=binomial)
table1 <- result(lg_model,test_data)
fraction(table1)
```

Lower than only use Lag2

### LDA
```{r}
lda_model <- lda(Direction ~ Lag2 + Volume +Lag1, data = train_data,family=binomial)
lda_pred <- predict(lda_model,test_data)$class
table2 <- table(lda_pred,test_data$Direction)
fraction(table2)
```

Lower than only use Lag2


### KNN
```{r}
Week.train <- as.matrix(train_data["Lag2"] + train_data["Volume"])
Week.test <- as.matrix(test_data["Lag2"] + test_data["Volume"])
train.Direction <- as.matrix(train_data["Direction"])
set.seed(1)
Weekknn.pred <- knn(Week.train,Week.test,train.Direction,k=4)
Weekknn_table <- table(Weekknn.pred,as.matrix(test_data["Direction"]))
Weekknn_table
fraction(Weekknn_table)


Week.train <- as.matrix(train_data["Lag2"] + train_data["Volume"])
Week.test <- as.matrix(test_data["Lag2"] + test_data["Volume"])
train.Direction <- as.matrix(train_data["Direction"])
set.seed(1)
Weekknn.pred <- knn(Week.train,Week.test,train.Direction,k=3)
Weekknn_table <- table(Weekknn.pred,as.matrix(test_data["Direction"]))
Weekknn_table
fraction(Weekknn_table)
Week.train <- as.matrix(train_data["Lag2"] + train_data["Volume"])
Week.test <- as.matrix(test_data["Lag2"] + test_data["Volume"])
train.Direction <- as.matrix(train_data["Direction"])
set.seed(1)
Weekknn.pred <- knn(Week.train,Week.test,train.Direction,k=2)
Weekknn_table <- table(Weekknn.pred,as.matrix(test_data["Direction"]))
Weekknn_table
fraction(Weekknn_table)
```

From the result we can see that the when K=3 and 4 the score is the best. 

### NaiveBayes
```{r}
model_nb <- naiveBayes(Week.train,train.Direction)
Weekknn_pred_nb <- predict(model_nb,Week.test)
Weekknn_table_nb <- table(Weekknn_pred_nb,as.matrix(test_data["Direction"]))

Weekknn_table_nb
fraction(Weekknn_table_nb)
```
Same with only using Lag2
